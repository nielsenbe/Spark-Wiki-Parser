{"paragraphs":[{"text":"%md\n###Overview\nThis notebook will walk you through converting a Wikipedia dump to Parquet files.\n\n###Configure the parser\nSee this page to add the correct dependencies to Zeppelin:\n[Wiki](https://github.com/nielsenbe/Spark-Wiki-Parser/wiki/Configure-Zeppelin)\n\nFor testing purposes it is recommended that you parse all elements.  The exception to this is the parseRefTags.  If reference templates are not a concern then go ahead and turn this one off as it does have an impact on performance.","dateUpdated":"2017-12-01T23:02:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Overview</h3>\n<p>This notebook will walk you through converting a Wikipedia dump to Parquet files.</p>\n<h3>Configure the parser</h3>\n<p>See this page to add the correct dependencies to Zeppelin:\n<br  /><a href=\"https://github.com/nielsenbe/Spark-Wiki-Parser/wiki/Configure-Zeppelin\">Wiki</a></p>\n<p>For testing purposes it is recommended that you parse all elements.  The exception to this is the parseRefTags.  If reference templates are not a concern then go ahead and turn this one off as it does have an impact on performance.</p>\n"}]},"apps":[],"jobName":"paragraph_1512169378123_-1808247105","id":"20171119-171014_1377154990","dateCreated":"2017-12-01T23:02:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1344"},{"text":"import main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia._\n\n// Settings and configuration\nval config = WkpParserConfiguration(\n    parseText = true, \n    parseTemplates = true, \n    parseLinks = true, \n    parseTags = true, \n    parseTables = true, \n    parseRefTags = false)\n    \n// Update file location info\n// S3 = s3://<bucketName>/<folders>\n// Azure = wasb://<containerName>@<storageAccountName>.blob.core.windows.net/<folders>\n// HDFS = hdfs://<folders>\nval folder = \"s3://bnielsenemr/\"\nval destFolder = \"wkpDb\"\nval sourceFile = \"enwiki-20171020-pages-articles.xml.bz2\"\n\n// Concat values\nval source = folder + sourceFile\nval destDB = folder + destFolder + \"/\"","dateUpdated":"2017-12-01T23:06:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512169378127_-1809786100","id":"20171119-170245_240537425","dateCreated":"2017-12-01T23:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1345","user":"anonymous","dateFinished":"2017-12-01T23:06:31+0000","dateStarted":"2017-12-01T23:06:29+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia._\nconfig: main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.WkpParserConfiguration = WkpParserConfiguration(true,true,true,true,true,false)\nfolder: String = s3://bnielsenemr/\ndestFolder: String = wkpDb\nsourceFile: String = enwiki-20171020-pages-articles.xml.bz2\nsource: String = s3://bnielsenemr/enwiki-20171020-pages-articles.xml.bz2\ndestDB: String = s3://bnielsenemr/wkpDb/\n"}]}},{"text":"%md\n### Read the XML\n\nThis next section reads the XML dump and converts it to Scala case classes.  No parsing is done in this step.  If you are getting errors about com.databricks.spark.xml then you may not have added the dependency correctly.  \nSee the top of the page for details on configuring Zeppelin.\n\nManually defining the xml saves us from having to spend time inferring it.","dateUpdated":"2017-12-01T23:02:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read the XML</h3>\n<p>This next section reads the XML dump and converts it to Scala case classes.  No parsing is done in this step.  If you are getting errors about com.databricks.spark.xml then you may not have added the dependency correctly.\n<br  />See the top of the page for details on configuring Zeppelin.</p>\n<p>Manually defining the xml saves us from having to spend time inferring it.</p>\n"}]},"apps":[],"jobName":"paragraph_1512169378127_-1809786100","id":"20171119-171738_504315396","dateCreated":"2017-12-01T23:02:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1346"},{"text":"import spark.implicits._\nimport org.apache.spark.sql.types._\n\n/* Dump Schema */\nval schema = new StructType()\n    .add(\"id\",LongType,true)\n    .add(\"ns\",LongType,true)\n    .add(\"restrictions\",StringType,true)\n    .add(\"title\",StringType,true)\n    .add(\"redirect\", new StructType()\n        .add(\"_VALUE\",StringType,true)\n        .add(\"_title\",StringType,true),\n        true)\n    .add(\"revision\", new StructType()\n        .add(\"comment\",StringType,true)\n        .add(\"format\",StringType,true)\n        .add(\"id\",LongType,true)\n        .add(\"minor\",StringType,true)\n        .add(\"model\",StringType,true)\n        .add(\"parentid\",LongType,true)\n        .add(\"sha1\",StringType,true)\n        .add(\"timestamp\",StringType,true)\n        .add(\"contributor\", new StructType()\n            .add(\"id\",LongType,true)\n            .add(\"ip\",StringType,true)\n            .add(\"username\",StringType, true),\n            true)\n        .add(\"text\", new StructType()\n            .add(\"_VALUE\",StringType,true)\n            .add(\"_space\",StringType,true),\n            true), \n        true)\n\n/* Read into dataset */\nval ds = spark.read\n    .format(\"com.databricks.spark.xml\")\n    .option(\"rowTag\", \"page\")\n    .schema(schema)\n    .load(source)\n    .as[InputPage]","dateUpdated":"2017-12-01T23:06:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512169378127_-1809786100","id":"20171118-213842_2146705316","dateCreated":"2017-12-01T23:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1347","user":"anonymous","dateFinished":"2017-12-01T23:06:39+0000","dateStarted":"2017-12-01T23:06:37+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\nimport org.apache.spark.sql.types._\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,true), StructField(ns,LongType,true), StructField(restrictions,StringType,true), StructField(title,StringType,true), StructField(redirect,StructType(StructField(_VALUE,StringType,true), StructField(_title,StringType,true)),true), StructField(revision,StructType(StructField(comment,StringType,true), StructField(format,StringType,true), StructField(id,LongType,true), StructField(minor,StringType,true), StructField(model,StringType,true), StructField(parentid,LongType,true), StructField(sha1,StringType,true), StructField(timestamp,StringType,true), StructField(contributor,StructType(StructField(id,LongType,true), StructField(ip,StringType,true), StructField(username,StringType,true)),true), StructField(text,...ds: org.apache.spark.sql.Dataset[main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.InputPage] = [id: bigint, ns: bigint ... 4 more fields]\n"}]}},{"text":"%md\n### Parse the wikitext and XML attributes\n\nThis section is where the parsing happens.  We use map Partitions for efficiency and serialization reasons.","dateUpdated":"2017-12-01T23:02:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Parse the wikitext and XML attributes</h3>\n<p>This section is where the parsing happens.  We use map Partitions for efficiency and serialization reasons.</p>\n"}]},"apps":[],"jobName":"paragraph_1512169378128_-1799397880","id":"20171119-171849_1860428245","dateCreated":"2017-12-01T23:02:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1348"},{"text":"val parsed = ds.mapPartitions(part => {\r\n    for(page <- part) yield WkpParser.parseWiki(page, config)\r\n})\r\n\r\nval items = parsed.cache\r\nval pageCount = items.count\r\npageCount","dateUpdated":"2017-12-01T23:06:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512169378128_-1799397880","id":"20171118-214653_1780397306","dateCreated":"2017-12-01T23:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1349","user":"anonymous","dateFinished":"2017-12-02T00:01:45+0000","dateStarted":"2017-12-01T23:06:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"parsed: org.apache.spark.sql.Dataset[main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.WikipediaPage] = [id: bigint, title: string ... 12 more fields]\nitems: parsed.type = [id: bigint, title: string ... 12 more fields]\npageCount: Long = 17953164\nres28: Long = 17953164\n"}]}},{"text":"%md\n### Check for Errors\nError messages are stored in the parserMessage field of the page object.","dateUpdated":"2017-12-01T23:02:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Check for Errors</h3>\n<p>Error messages are stored in the parserMessage field of the page object.</p>\n"}]},"apps":[],"jobName":"paragraph_1512169378128_-1799397880","id":"20171121-220449_468004919","dateCreated":"2017-12-01T23:02:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1350"},{"text":"// Check for pages that failed parsing\nval errors = items.filter(_.parserMessage != \"SUCCESS\")\nval errorCount = errors.count\n\nprintln(s\" $errorCount out of $pageCount failed\")\n\nerrors.toDF.select(\"id\", \"title\", \"parserMessage\").show","dateUpdated":"2017-12-02T00:02:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512169378128_-1799397880","id":"20171121-214950_220274022","dateCreated":"2017-12-01T23:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1351","user":"anonymous","dateFinished":"2017-12-02T00:04:18+0000","dateStarted":"2017-12-02T00:02:55+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"errors: org.apache.spark.sql.Dataset[main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.WikipediaPage] = [id: bigint, title: string ... 12 more fields]\nerrorCount: Long = 1480\n 1480 out of 17953164 failed\n+-------+--------------------+-------------+\n|     id|               title|parserMessage|\n+-------+--------------------+-------------+\n| 551027|Wikipedia:WikiPro...|  Error: null|\n| 551039|Wikipedia:WikiPro...|  Error: null|\n| 551042|Wikipedia:WikiPro...|  Error: null|\n| 551056|Wikipedia:WikiPro...|  Error: null|\n| 551062|Wikipedia:WikiPro...|  Error: null|\n| 551073|Wikipedia:WikiPro...|  Error: null|\n| 551078|Wikipedia:WikiPro...|  Error: null|\n|1116672|Wikipedia:WikiPro...|  Error: null|\n|1215455|Wikipedia:Pages n...|  Error: null|\n|1238000|Wikipedia:News so...|  Error: null|\n|1238097|Wikipedia:News so...|  Error: null|\n|1238104|Wikipedia:News so...|  Error: null|\n|1238135|Wikipedia:News so...|  Error: null|\n|1238142|Wikipedia:News so...|  Error: null|\n|1238218|Wikipedia:News so...|  Error: null|\n|1252950|Wikipedia:Article...|  Error: null|\n|1450531|Wikipedia:News so...|  Error: null|\n|1632663|Wikipedia:WikiPro...|  Error: null|\n|2054579|Wikipedia:WikiPro...|  Error: null|\n|2054603|Wikipedia:WikiPro...|  Error: null|\n+-------+--------------------+-------------+\nonly showing top 20 rows\n\n"}]}},{"text":"%md\n###Export results to Parquet files\nConvert the Scala case classes to Parquet files.  Parquet files are a preferred format in Spark because they are highly compressed and columnar.\n\nComment out any parts you do not care about to save on space.  If you intend to follow the rest of the notebooks then do leave all of the parts in.\n\nThe Wikipedia domain classes can just as easily be saved to text, JSON, Hive, or JDBC.","dateUpdated":"2017-12-01T23:02:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Export results to Parquet files</h3>\n<p>Convert the Scala case classes to Parquet files.  Parquet files are a preferred format in Spark because they are highly compressed and columnar.</p>\n<p>Comment out any parts you do not care about to save on space.  If you intend to follow the rest of the notebooks then do leave all of the parts in.</p>\n<p>The Wikipedia domain classes can just as easily be saved to text, JSON, Hive, or JDBC.</p>\n"}]},"apps":[],"jobName":"paragraph_1512169378129_-1799782629","id":"20171119-172417_1507925883","dateCreated":"2017-12-01T23:02:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1352"},{"text":"/* Split the pages and redirects into two different files */\n/* Write pages */\nitems.createOrReplaceTempView(\"pages\")\nval pages = spark.sql(\"\"\"\nSELECT\n  id,\n  title,\n  redirect,\n  nameSpace,\n  pageType,\n  lastRevisionId,\n  lastRevisionDate\nFROM\n    pages\nWHERE redirect = ''\nAND   parserMessage = 'SUCCESS'\n\"\"\")\n\npages\n    .write\n    .mode(\"Overwrite\")\n    .bucketBy(40, \"id\")\n    .sortBy(\"id\")\n    .saveAsTable(destDB + \"wkp_pages.parquet\")\n\n/* Write redirects */\nval redirects = spark.sql(\"\"\"\nSELECT\n    art.id      AS targetPageId,\n    rdr.id      AS redirectPageId,\n    rdr.title   AS redirectTitle\nFROM\n    pages rdr\nJOIN\n    pages art\n    ON  art.title = rdr.redirect\n\"\"\")\n\nredirects\n    .write\n    .mode(\"Overwrite\")\n    .bucketBy(40, \"id\")\n    .sortBy(\"id\")\n    .saveAsTable(destDB + \"wkp_redirects.parquet\")\n\n/* Write page elements */\nitems.flatMap(_.headerSections).write.mode(\"Overwrite\").parquet(destDB +  \"wkp_headers.parquet\")\nitems.flatMap(_.links).write.mode(\"Overwrite\").parquet(destDB + \"wkp_links.parquet\")\nitems.flatMap(_.texts).write.mode(\"Overwrite\").parquet(destDB + \"wkp_text.parquet\")\nitems.flatMap(_.tags).write.mode(\"Overwrite\").parquet(destDB + \"wkp_tags.parquet\")\nitems.flatMap(_.tables).write.mode(\"Overwrite\").parquet(destDB + \"wkp_tables.parquet\")\n\n\n/* Flatten templates */\nitems.flatMap(_.templates).createOrReplaceTempView(\"templates\")\nval flatTemplates = spark.sql(\"SELECT parentPageId, parentHeaderId, elementId, templateType, isInfoBox FROM templates\")\nval flatTemplateParameters = spark.sql(\"SELECT tmp.parentPageId, tmp.elementId, params._1 AS paramName, params._2 AS paramValue FROM templates TMP LATERAL VIEW explode(parameters) AS params\")\nflatTemplates.write.mode(\"Overwrite\").parquet(destDB + \"wkp_templates.parquet\")\nflatTemplates.write.mode(\"Overwrite\").parquet(destDB + \"wkp_template_parameters.parquet\")","dateUpdated":"2017-12-01T23:52:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512169378129_-1799782629","id":"20171118-225216_745240947","dateCreated":"2017-12-01T23:02:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1353"},{"text":"// If you are going to immediatly analyze the data then it is best to clear the parser cache\n\nitems.unpersist","dateUpdated":"2017-12-01T23:02:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512169378129_-1799782629","id":"20171128-010321_1938067778","dateCreated":"2017-12-01T23:02:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1354"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512171610431_1537948541","id":"20171201-234010_1768720119","dateCreated":"2017-12-01T23:40:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2366","text":"items.flatMap(_.templates).createOrReplaceTempView(\"templates\")\nval flatTemplates = spark.sql(\"SELECT parentPageId, parentHeaderId, elementId, templateType, isInfoBox FROM templates\")\nval flatTemplateParameters = spark.sql(\"SELECT tmp.parentPageId, tmp.elementId, params._1 AS paramName, params._2 AS paramValue FROM templates TMP LATERAL VIEW explode(parameters) AS params\")\nflatTemplates.write.mode(\"Overwrite\").parquet(destDB + \"wkp_templates.parquet\")\nflatTemplates.write.mode(\"Overwrite\").parquet(destDB + \"wkp_template_parameters.parquet\")","dateUpdated":"2017-12-02T00:29:25+0000","dateFinished":"2017-12-02T00:32:16+0000","dateStarted":"2017-12-02T00:29:25+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"flatTemplates: org.apache.spark.sql.DataFrame = [parentPageId: int, parentHeaderId: int ... 3 more fields]\nflatTemplateParameters: org.apache.spark.sql.DataFrame = [parentPageId: int, elementId: int ... 2 more fields]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512173482553_488371033","id":"20171202-001122_1486539251","dateCreated":"2017-12-02T00:11:22+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2721"}],"name":"Scala - Process Wikipedia Dump","id":"2CYUDJG84","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}