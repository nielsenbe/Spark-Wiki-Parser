{"paragraphs":[{"text":"%md\n###Overview\nThis notebook will walk you through converting a Wikipedia dump to Parquet files.\n\n###Configure the parser\nSee this page to add the correct dependencies to Zeppelin:\n[Wiki](https://github.com/nielsenbe/Spark-Wiki-Parser/wiki/Configure-Zeppelin)\n\nFor testing purposes it is recommended that you parse all elements.  The exception to this is the parseRefTags.  If reference templates are not a concern then go ahead and turn this one off as it does have an impact on performance.\n\nThis notebook assumes you are hosting your files in S3.  Simply paste in your appropriate S3 info.  If you are having access denied errors then reference this page: [S3 EMR Permissions](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-iam-roles.html)","dateUpdated":"2017-11-28T03:58:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Overview</h3>\n<p>This notebook will walk you through converting a Wikipedia dump to Parquet files.</p>\n<h3>Configure the parser</h3>\n<p>See this page to add the correct dependencies to Zeppelin:\n<br  /><a href=\"https://github.com/nielsenbe/Spark-Wiki-Parser/wiki/Configure-Zeppelin\">Wiki</a></p>\n<p>For testing purposes it is recommended that you parse all elements.  The exception to this is the parseRefTags.  If reference templates are not a concern then go ahead and turn this one off as it does have an impact on performance.</p>\n<p>This notebook assumes you are hosting your files in S3.  Simply paste in your appropriate S3 info.  If you are having access denied errors then reference this page: <a href=\"http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-iam-roles.html\">S3 EMR Permissions</a></p>\n"}]},"apps":[],"jobName":"paragraph_1511841502725_-325351185","id":"20171119-171014_1377154990","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2430"},{"text":"import main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia._\n\n// Settings and configuration\nval config = WkpParserConfiguration(\n    parseText = true, \n    parseTemplates = true, \n    parseLinks = true, \n    parseTags = true, \n    parseTables = true, \n    parseRefTags = false)\n    \n// Update S3 Info\nval bucketName = \"bnielsenemr\"\nval destFolder = \"wkpDb\"\nval sourceFile = \"enwiki-20171020-pages-articles.xml.bz2\"\n\n// Concat values\nval s3 = s\"s3://$bucketName/\"\nval source = s3 + sourceFile\nval destDB = s3 + destFolder + \"/\"","dateUpdated":"2017-11-28T03:58:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia._\nconfig: main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.WkpParserConfiguration = WkpParserConfiguration(true,true,true,true,true,false)\nbucketName: String = bnielsenemr\ndestFolder: String = wkpDb\nsourceFile: String = enwiki-20171020-pages-articles.xml.bz2\ns3: String = s3://bnielsenemr/\nsource: String = s3://bnielsenemr/enwiki-20171020-pages-articles.xml.bz2\ndestDB: String = s3://bnielsenemr/wkpDb/\n"}]},"apps":[],"jobName":"paragraph_1511841502726_-324196939","id":"20171119-170245_240537425","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2431"},{"text":"%md\n### Read the XML\n\nThis next section reads the XML dump and converts it to Scala case classes.  No parsing is done in this step.  If you are getting errors about com.databricks.spark.xml then you may not have added the dependency correctly.  \nSee the top of the page for details on configuring Zeppelin.\n\nManually defining the xml saves us from having to spend time inferring it.","dateUpdated":"2017-11-28T03:58:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read the XML</h3>\n<p>This next section reads the XML dump and converts it to Scala case classes.  No parsing is done in this step.  If you are getting errors about com.databricks.spark.xml then you may not have added the dependency correctly.\n<br  />See the top of the page for details on configuring Zeppelin.</p>\n<p>Manually defining the xml saves us from having to spend time inferring it.</p>\n"}]},"apps":[],"jobName":"paragraph_1511841502726_-324196939","id":"20171119-171738_504315396","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2432"},{"text":"import spark.implicits._\nimport org.apache.spark.sql.types._\n\n/* Dump Schema */\nval schema = new StructType()\n    .add(\"id\",LongType,true)\n    .add(\"ns\",LongType,true)\n    .add(\"restrictions\",StringType,true)\n    .add(\"title\",StringType,true)\n    .add(\"redirect\", new StructType()\n        .add(\"_VALUE\",StringType,true)\n        .add(\"_title\",StringType,true),\n        true)\n    .add(\"revision\", new StructType()\n        .add(\"comment\",StringType,true)\n        .add(\"format\",StringType,true)\n        .add(\"id\",LongType,true)\n        .add(\"minor\",StringType,true)\n        .add(\"model\",StringType,true)\n        .add(\"parentid\",LongType,true)\n        .add(\"sha1\",StringType,true)\n        .add(\"timestamp\",StringType,true)\n        .add(\"contributor\", new StructType()\n            .add(\"id\",LongType,true)\n            .add(\"ip\",StringType,true)\n            .add(\"username\",StringType, true),\n            true)\n        .add(\"text\", new StructType()\n            .add(\"_VALUE\",StringType,true)\n            .add(\"_space\",StringType,true),\n            true), \n        true)\n\n/* Read into dataset */\nval ds = spark.read\n    .format(\"com.databricks.spark.xml\")\n    .option(\"rowTag\", \"page\")\n    .schema(schema)\n    .load(source)\n    .as[InputPage]","dateUpdated":"2017-11-28T03:58:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\nimport org.apache.spark.sql.types._\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,true), StructField(ns,LongType,true), StructField(restrictions,StringType,true), StructField(title,StringType,true), StructField(redirect,StructType(StructField(_VALUE,StringType,true), StructField(_title,StringType,true)),true), StructField(revision,StructType(StructField(comment,StringType,true), StructField(format,StringType,true), StructField(id,LongType,true), StructField(minor,StringType,true), StructField(model,StringType,true), StructField(parentid,LongType,true), StructField(sha1,StringType,true), StructField(timestamp,StringType,true), StructField(contributor,StructType(StructField(id,LongType,true), StructField(ip,StringType,true), StructField(username,StringType,true)),true), StructField(text,...ds: org.apache.spark.sql.Dataset[main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.InputPage] = [id: bigint, ns: bigint ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1511841502726_-324196939","id":"20171118-213842_2146705316","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2433"},{"text":"%md\n### Parse the wikitext and XML attributes\n\nThis section is where the parsing happens.  We use map Partitions for efficiency and serialization reasons.","dateUpdated":"2017-11-28T03:58:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Parse the wikitext and XML attributes</h3>\n<p>This section is where the parsing happens.  We use map Partitions for efficiency and serialization reasons.</p>\n"}]},"apps":[],"jobName":"paragraph_1511841502726_-324196939","id":"20171119-171849_1860428245","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2434"},{"text":"val parsed = ds.mapPartitions(part => {\r\n    for(page <- part) yield WkpParser.parseWiki(page, config)\r\n})\r\n\r\nval items = parsed.cache\r\nval pageCount = items.count\r\narticleCount","dateUpdated":"2017-11-28T04:05:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"parsed: org.apache.spark.sql.Dataset[main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.WikipediaArticle] = [id: bigint, title: string ... 12 more fields]\nitems: parsed.type = [id: bigint, title: string ... 12 more fields]\narticleCount: Long = 17953164\nres42: Long = 17953164\n"}]},"apps":[],"jobName":"paragraph_1511841502727_-324581687","id":"20171118-214653_1780397306","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2435"},{"text":"%md\n### Check for Errors","dateUpdated":"2017-11-28T03:58:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Check for Errors</h3>\n"}]},"apps":[],"jobName":"paragraph_1511841502727_-324581687","id":"20171121-220449_468004919","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2436"},{"text":"// Check for pages that failed parsing\nval errors = items.filter(_.parserMessage != \"SUCCESS\")\nval errorCount = errors.count\n\nprintln(s\" $errorCount out of $pageCount failed\")\n\nerrors.toDF.select(\"id\", \"title\", \"parserMessage\").show","dateUpdated":"2017-11-28T04:05:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"errors: org.apache.spark.sql.Dataset[main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.WikipediaArticle] = [id: bigint, title: string ... 12 more fields]\nerrorCount: Long = 1480\n 1480 out of 17953164 failed\n+-------+--------------------+--------------------+\n|     id|               title|       parserMessage|\n+-------+--------------------+--------------------+\n| 551027|Wikipedia:WikiPro...|Error: Validation...|\n| 551039|Wikipedia:WikiPro...|Error: Validation...|\n| 551042|Wikipedia:WikiPro...|Error: Validation...|\n| 551056|Wikipedia:WikiPro...|Error: Validation...|\n| 551062|Wikipedia:WikiPro...|Error: Validation...|\n| 551073|Wikipedia:WikiPro...|Error: Validation...|\n| 551078|Wikipedia:WikiPro...|Error: Validation...|\n|1116672|Wikipedia:WikiPro...|Error: Validation...|\n|1215455|Wikipedia:Pages n...|Error: Validation...|\n|1238000|Wikipedia:News so...|Error: Validation...|\n|1238097|Wikipedia:News so...|Error: Validation...|\n|1238104|Wikipedia:News so...|Error: Validation...|\n|1238135|Wikipedia:News so...|Error: Validation...|\n|1238142|Wikipedia:News so...|Error: Validation...|\n|1238218|Wikipedia:News so...|Error: Validation...|\n|1252950|Wikipedia:Article...|Error: Validation...|\n|1450531|Wikipedia:News so...|Error: Validation...|\n|1632663|Wikipedia:WikiPro...|Error: Validation...|\n|2054579|Wikipedia:WikiPro...|Error: Validation...|\n|2054603|Wikipedia:WikiPro...|Error: Validation...|\n+-------+--------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1511841502727_-324581687","id":"20171121-214950_220274022","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2437"},{"text":"%md\n###Export results to Parquet files\nConvert the Scala case classes to Parquet files.  Parquet files are a preferred format in Spark because they are highly compressed and columnar.\n\nComment out any parts you do not care about to save on space.","dateUpdated":"2017-11-28T03:58:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Export results to Parquet files</h3>\n<p>Convert the Scala case classes to Parquet files.  Parquet files are a preferred format in Spark because they are highly compressed and columnar.</p>\n<p>Comment out any parts you do not care about to save on space.</p>\n"}]},"apps":[],"jobName":"paragraph_1511841502727_-324581687","id":"20171119-172417_1507925883","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2438"},{"text":"/* Write pages */\nitems.createOrReplaceTempView(\"pages\")\nval pages = spark.sql(\"\"\"\nSELECT\n  id,\n  title,\n  redirect,\n  nameSpace,\n  pageType,\n  lastRevisionId,\n  lastRevisionDate\nFROM\n    pages\nWHERE redirect = ''\nAND   parserMessage = 'SUCCESS'\n\"\"\")\n\npages\n    .write\n    .mode(\"Overwrite\")\n    .parquet(destDB + \"wkp_paged.parquet\")\n\n/* Write redirects */\nval redirects = spark.sql(\"\"\"\nSELECT\n    art.id      AS targetPageId,\n    rdr.id      AS redirectPageId,\n    rdr.title   AS redirectTitle\nFROM\n    pages rdr\nJOIN\n    pages art\n    ON  art.title = rdr.redirect\n\"\"\")\n\nredirects\n    .write\n    .mode(\"Overwrite\")\n    .parquet(destDB + \"wkp_redirects.parquet\")\n\n/* Write article elements */\nitems.flatMap(_.headerSections).write.mode(\"Overwrite\").parquet(destDB +  \"wkp_headers.parquet\")\nitems.flatMap(_.links).write.mode(\"Overwrite\").parquet(destDB + \"wkp_links.parquet\")\nitems.flatMap(_.texts).write.mode(\"Overwrite\").parquet(destDB + \"wkp_text.parquet\")\nitems.flatMap(_.tags).write.mode(\"Overwrite\").parquet(destDB + \"wkp_tags.parquet\")\nitems.flatMap(_.tables).write.mode(\"Overwrite\").parquet(destDB + \"wkp_tables.parquet\")\n\n\n/* Flatten templates */\nitems.flatMap(_.templates).createOrReplaceTempView(\"templates\")\nval flatTemplates = spark.sql(\"SELECT parentArticleId, parentHeaderId, elementId, templateType, isInfoBox FROM templates\")\nval flatTemplateParameters = spark.sql(\"SELECT tmp.parentArticleId, tmp.elementId, params._1 AS paramName, params._2 AS paramValue FROM templates TMP LATERAL VIEW explode(parameters) AS params\")\nflatTemplates.write.mode(\"Overwrite\").parquet(destDB + \"wkp_templates.parquet\")\nflatTemplates.write.mode(\"Overwrite\").parquet(destDB + \"wkp_template_parameters.parquet\")","dateUpdated":"2017-11-28T04:06:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"articles: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 5 more fields]\nredirects: org.apache.spark.sql.DataFrame = [targetPageId: bigint, redirectPageId: bigint ... 1 more field]\n<console>:4: error: identifier expected but '.' found.\nitems.flatMap(_.headerSections).write..mode(\"Overwrite\").parquet(destDB +  \"wkp_headers.parquet\")\n                                      ^\n"}]},"apps":[],"jobName":"paragraph_1511841502728_-326505432","id":"20171118-225216_745240947","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2439"},{"text":"// If you are going to immediatly analyze the data then it is best to clear the parser cache\n\nitems.unpersist","dateUpdated":"2017-11-28T03:58:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511841502728_-326505432","id":"20171128-010321_1938067778","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2440"},{"text":"%spark.sql\n","dateUpdated":"2017-11-28T03:58:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511841502728_-326505432","id":"20171128-013642_123181573","dateCreated":"2017-11-28T03:58:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2441"}],"name":"EMR - Scala - Process Wikipedia Dump","id":"2D17A1TPZ","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}